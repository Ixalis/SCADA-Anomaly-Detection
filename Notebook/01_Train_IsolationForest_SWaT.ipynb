{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Train and Save Isolation Forest Model\n",
    "## SWaT Dataset - SCADA Anomaly Detection\n",
    "\n",
    "**Purpose:** Train the model ONCE and save it for reuse\n",
    "\n",
    "**Run this notebook:**\n",
    "- Once initially\n",
    "- When you want to retrain with new data\n",
    "- When SCADA operations change significantly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"Google Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================\n",
    "\n",
    "# Path to your SWaT training data (normal operations)\n",
    "TRAIN_DATA_PATH = '/content/drive/MyDrive/4th7thsemproject/SWaT_Normal.csv'\n",
    "\n",
    "# Path to save the trained model\n",
    "MODEL_SAVE_PATH = '/content/drive/MyDrive/4th7thsemproject/models/isolation_forest_model.pkl'\n",
    "\n",
    "# Path to save the scaler (IMPORTANT - needed for predictions!)\n",
    "SCALER_SAVE_PATH = '/content/drive/MyDrive/4th7thsemproject/models/scaler.pkl'\n",
    "\n",
    "# Path to save feature column names\n",
    "FEATURES_SAVE_PATH = '/content/drive/MyDrive/4th7thsemproject/models/feature_columns.pkl'\n",
    "\n",
    "# =============================================================\n",
    "# MODEL PARAMETERS\n",
    "# =============================================================\n",
    "\n",
    "# Contamination: Expected proportion of anomalies in training data\n",
    "# For SWaT normal data, this should be very low (0.01 = 1%)\n",
    "CONTAMINATION = 0.01\n",
    "\n",
    "# Number of trees in the forest\n",
    "N_ESTIMATORS = 100\n",
    "\n",
    "# Random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# =============================================================\n",
    "# COLUMN CONFIGURATION\n",
    "# =============================================================\n",
    "\n",
    "# Columns to EXCLUDE from training (not sensor data)\n",
    "EXCLUDE_COLUMNS = [\n",
    "    'Timestamp',\n",
    "    ' Timestamp',\n",
    "    'datetime',\n",
    "    'Normal/Attack',\n",
    "    ' Normal/Attack',\n",
    "    'label',\n",
    "    'Label',\n",
    "    'Attack',\n",
    "]\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Training data: {TRAIN_DATA_PATH}\")\n",
    "print(f\"Model will be saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "print(f\"Loading training data from: {TRAIN_DATA_PATH}\")\n",
    "print(\"This may take a moment for large files...\")\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "print(f\"   Shape: {train_df.shape[0]:,} rows √ó {train_df.shape[1]} columns\")\n",
    "print(f\"   Memory usage: {train_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "print(\"First 5 rows:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the label column (if present)\n",
    "label_col = None\n",
    "for col in ['Normal/Attack', ' Normal/Attack', 'label', 'Label', 'Attack', 'attack']:\n",
    "    if col in train_df.columns:\n",
    "        label_col = col\n",
    "        print(f\"Found label column: '{label_col}'\")\n",
    "        print(f\"Value counts:\")\n",
    "        print(train_df[label_col].value_counts())\n",
    "        break\n",
    "\n",
    "if label_col is None:\n",
    "    print(\"No label column found - assuming all data is normal (which is correct for training!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns (exclude non-sensor columns)\n",
    "feature_columns = [col for col in train_df.columns if col not in EXCLUDE_COLUMNS]\n",
    "\n",
    "# Also exclude any non-numeric columns\n",
    "numeric_features = train_df[feature_columns].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Total columns: {len(train_df.columns)}\")\n",
    "print(f\"Feature columns selected: {len(numeric_features)}\")\n",
    "print(f\"\\nFeatures to be used for training:\")\n",
    "for i, col in enumerate(numeric_features, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature matrix\n",
    "X_train = train_df[numeric_features].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train.shape}\")\n",
    "print(f\"  - {X_train.shape[0]:,} samples\")\n",
    "print(f\"  - {X_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = np.isnan(X_train).sum()\n",
    "if missing > 0:\n",
    "    print(f\"‚ö†Ô∏è Found {missing} missing values\")\n",
    "    print(\"Filling with column means...\")\n",
    "    col_means = np.nanmean(X_train, axis=0)\n",
    "    nan_indices = np.where(np.isnan(X_train))\n",
    "    X_train[nan_indices] = np.take(col_means, nan_indices[1])\n",
    "    print(\"‚úÖ Missing values handled\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the scaler\n",
    "print(\"Fitting StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "print(\"‚úÖ Scaler fitted!\")\n",
    "print(f\"\\nScaling statistics (first 5 features):\")\n",
    "for i in range(min(5, len(numeric_features))):\n",
    "    print(f\"  {numeric_features[i]}: mean={scaler.mean_[i]:.4f}, std={scaler.scale_[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train Isolation Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "print(\"Initializing Isolation Forest model...\")\n",
    "print(f\"  - n_estimators: {N_ESTIMATORS}\")\n",
    "print(f\"  - contamination: {CONTAMINATION}\")\n",
    "print(f\"  - random_state: {RANDOM_STATE}\")\n",
    "\n",
    "clf = IsolationForest(\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    contamination=CONTAMINATION,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Training model... (this may take a few minutes)\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "clf.fit(X_train_scaled)\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n‚úÖ Model trained successfully!\")\n",
    "print(f\"   Training time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation on training data\n",
    "print(\"Running quick validation on training data...\")\n",
    "train_predictions = clf.predict(X_train_scaled)\n",
    "\n",
    "n_normal = (train_predictions == 1).sum()\n",
    "n_anomaly = (train_predictions == -1).sum()\n",
    "\n",
    "print(f\"\\nTraining data predictions:\")\n",
    "print(f\"  Normal:  {n_normal:,} ({n_normal/len(train_predictions)*100:.2f}%)\")\n",
    "print(f\"  Anomaly: {n_anomaly:,} ({n_anomaly/len(train_predictions)*100:.2f}%)\")\n",
    "print(f\"\\nExpected anomaly rate: {CONTAMINATION*100:.2f}%\")\n",
    "print(f\"Actual anomaly rate:   {n_anomaly/len(train_predictions)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Model, Scaler, and Feature Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the models directory if it doesn't exist\n",
    "model_dir = os.path.dirname(MODEL_SAVE_PATH)\n",
    "if model_dir and not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    print(f\"Created directory: {model_dir}\")\n",
    "\n",
    "# Save the trained model\n",
    "print(f\"\\nSaving model to: {MODEL_SAVE_PATH}\")\n",
    "joblib.dump(clf, MODEL_SAVE_PATH)\n",
    "print(\"‚úÖ Model saved!\")\n",
    "\n",
    "# Save the scaler\n",
    "print(f\"\\nSaving scaler to: {SCALER_SAVE_PATH}\")\n",
    "joblib.dump(scaler, SCALER_SAVE_PATH)\n",
    "print(\"‚úÖ Scaler saved!\")\n",
    "\n",
    "# Save feature column names\n",
    "print(f\"\\nSaving feature columns to: {FEATURES_SAVE_PATH}\")\n",
    "joblib.dump(numeric_features, FEATURES_SAVE_PATH)\n",
    "print(\"‚úÖ Feature columns saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the saved files\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVED FILES VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "files_to_check = [\n",
    "    (\"Model\", MODEL_SAVE_PATH),\n",
    "    (\"Scaler\", SCALER_SAVE_PATH),\n",
    "    (\"Features\", FEATURES_SAVE_PATH)\n",
    "]\n",
    "\n",
    "for name, path in files_to_check:\n",
    "    if os.path.exists(path):\n",
    "        size = os.path.getsize(path) / 1024\n",
    "        print(f\"‚úÖ {name}: {path}\")\n",
    "        print(f\"   Size: {size:.2f} KB\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name}: FILE NOT FOUND - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"   - Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"   - Features used: {X_train.shape[1]}\")\n",
    "print(f\"\\nü§ñ Model Configuration:\")\n",
    "print(f\"   - Algorithm: Isolation Forest\")\n",
    "print(f\"   - Trees: {N_ESTIMATORS}\")\n",
    "print(f\"   - Contamination: {CONTAMINATION}\")\n",
    "print(f\"   - Training time: {training_time:.2f} seconds\")\n",
    "print(f\"\\nüíæ Saved Files:\")\n",
    "print(f\"   - Model: {MODEL_SAVE_PATH}\")\n",
    "print(f\"   - Scaler: {SCALER_SAVE_PATH}\")\n",
    "print(f\"   - Features: {FEATURES_SAVE_PATH}\")\n",
    "print(f\"\\n‚úÖ Next Step: Run '02_Inference_IsolationForest_SWaT.ipynb' to make predictions!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
