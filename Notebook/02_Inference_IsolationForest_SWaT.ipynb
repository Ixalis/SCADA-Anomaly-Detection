{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Load Model and Make Predictions\n",
    "## SWaT Dataset - SCADA Anomaly Detection\n",
    "\n",
    "**Purpose:** Load the pre-trained model and make predictions on test data\n",
    "\n",
    "**Prerequisites:** Must have run `01_Train_IsolationForest_SWaT.ipynb` first!\n",
    "\n",
    "**Run this notebook:**\n",
    "- To test on attack dataset\n",
    "- Daily for new data\n",
    "- Whenever you need predictions (fast - no retraining!)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"Google Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================\n",
    "\n",
    "# Path to your SWaT TEST data (contains attacks)\n",
    "TEST_DATA_PATH = '/content/drive/MyDrive/4th7thsemproject/SWaT_Attack.csv'\n",
    "\n",
    "# Paths to saved model artifacts (must match training notebook!)\n",
    "MODEL_PATH = '/content/drive/MyDrive/4th7thsemproject/models/isolation_forest_model.pkl'\n",
    "SCALER_PATH = '/content/drive/MyDrive/4th7thsemproject/models/scaler.pkl'\n",
    "FEATURES_PATH = '/content/drive/MyDrive/4th7thsemproject/models/feature_columns.pkl'\n",
    "\n",
    "# Path to save prediction results\n",
    "RESULTS_SAVE_PATH = '/content/drive/MyDrive/4th7thsemproject/results/predictions.csv'\n",
    "\n",
    "# =============================================================\n",
    "# LABEL CONFIGURATION (for evaluation)\n",
    "# =============================================================\n",
    "\n",
    "# How labels are encoded in your test data\n",
    "NORMAL_LABEL = 'Normal'\n",
    "ATTACK_LABEL = 'Attack'\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"Test data: {TEST_DATA_PATH}\")\n",
    "print(f\"Model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Saved Model, Scaler, and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify files exist before loading\n",
    "print(\"Checking for saved model artifacts...\\n\")\n",
    "\n",
    "files_to_check = [\n",
    "    (\"Model\", MODEL_PATH),\n",
    "    (\"Scaler\", SCALER_PATH),\n",
    "    (\"Features\", FEATURES_PATH)\n",
    "]\n",
    "\n",
    "all_files_exist = True\n",
    "for name, path in files_to_check:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"‚úÖ {name}: Found\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name}: NOT FOUND - {path}\")\n",
    "        all_files_exist = False\n",
    "\n",
    "if not all_files_exist:\n",
    "    raise FileNotFoundError(\"\\n‚ö†Ô∏è Missing model files! Run '01_Train_IsolationForest_SWaT.ipynb' first!\")\n",
    "\n",
    "print(\"\\n‚úÖ All model files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "print(\"Loading model...\")\n",
    "clf = joblib.load(MODEL_PATH)\n",
    "print(f\"‚úÖ Model loaded: {type(clf).__name__}\")\n",
    "print(f\"   - n_estimators: {clf.n_estimators}\")\n",
    "print(f\"   - contamination: {clf.contamination}\")\n",
    "\n",
    "# Load the scaler\n",
    "print(\"\\nLoading scaler...\")\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "print(f\"‚úÖ Scaler loaded: {type(scaler).__name__}\")\n",
    "\n",
    "# Load feature column names\n",
    "print(\"\\nLoading feature columns...\")\n",
    "feature_columns = joblib.load(FEATURES_PATH)\n",
    "print(f\"‚úÖ Feature columns loaded: {len(feature_columns)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "print(f\"Loading test data from: {TEST_DATA_PATH}\")\n",
    "print(\"This may take a moment for large files...\")\n",
    "\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "print(f\"\\n‚úÖ Test data loaded successfully!\")\n",
    "print(f\"   Shape: {test_df.shape[0]:,} rows √ó {test_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview test data\n",
    "print(\"First 5 rows:\")\n",
    "display(test_df.head())\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and extract labels (ground truth)\n",
    "label_col = None\n",
    "for col in ['Normal/Attack', ' Normal/Attack', 'label', 'Label', 'Attack', 'attack']:\n",
    "    if col in test_df.columns:\n",
    "        label_col = col\n",
    "        break\n",
    "\n",
    "if label_col:\n",
    "    print(f\"Found label column: '{label_col}'\")\n",
    "    print(f\"\\nValue counts:\")\n",
    "    print(test_df[label_col].value_counts())\n",
    "    \n",
    "    # Store ground truth\n",
    "    y_true_raw = test_df[label_col].values\n",
    "    \n",
    "    # Convert to binary (1 = Normal, -1 = Anomaly) to match Isolation Forest output\n",
    "    y_true = np.where(\n",
    "        (y_true_raw == NORMAL_LABEL) | (y_true_raw == 'Normal') | (y_true_raw == 0),\n",
    "        1,   # Normal\n",
    "        -1   # Anomaly/Attack\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nConverted labels:\")\n",
    "    print(f\"  Normal (1):  {(y_true == 1).sum():,}\")\n",
    "    print(f\"  Anomaly (-1): {(y_true == -1).sum():,}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No label column found - will only show predictions without evaluation\")\n",
    "    y_true = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Prepare Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all expected features are present\n",
    "missing_features = [f for f in feature_columns if f not in test_df.columns]\n",
    "if missing_features:\n",
    "    print(f\"‚ö†Ô∏è Missing features in test data: {missing_features}\")\n",
    "    raise ValueError(\"Test data is missing required features!\")\n",
    "else:\n",
    "    print(f\"‚úÖ All {len(feature_columns)} features found in test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature matrix (using same columns as training!)\n",
    "X_test = test_df[feature_columns].values\n",
    "\n",
    "print(f\"Test feature matrix shape: {X_test.shape}\")\n",
    "print(f\"  - {X_test.shape[0]:,} samples\")\n",
    "print(f\"  - {X_test.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (same as training)\n",
    "missing = np.isnan(X_test).sum()\n",
    "if missing > 0:\n",
    "    print(f\"‚ö†Ô∏è Found {missing} missing values\")\n",
    "    print(\"Filling with training means (from scaler)...\")\n",
    "    nan_indices = np.where(np.isnan(X_test))\n",
    "    X_test[nan_indices] = np.take(scaler.mean_, nan_indices[1])\n",
    "    print(\"‚úÖ Missing values handled\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale test data using the SAVED scaler (important!)\n",
    "print(\"Scaling test data with saved scaler...\")\n",
    "X_test_scaled = scaler.transform(X_test)  # Use transform, NOT fit_transform!\n",
    "print(\"‚úÖ Test data scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"üîÆ Making predictions...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "end_time = datetime.now()\n",
    "prediction_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"‚úÖ Predictions complete!\")\n",
    "print(f\"   Time: {prediction_time:.2f} seconds\")\n",
    "print(f\"   Speed: {len(y_pred)/prediction_time:,.0f} predictions/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get anomaly scores (more detailed than binary prediction)\n",
    "anomaly_scores = clf.decision_function(X_test_scaled)\n",
    "\n",
    "print(\"Prediction summary:\")\n",
    "print(f\"  Normal (1):   {(y_pred == 1).sum():,} ({(y_pred == 1).sum()/len(y_pred)*100:.2f}%)\")\n",
    "print(f\"  Anomaly (-1): {(y_pred == -1).sum():,} ({(y_pred == -1).sum()/len(y_pred)*100:.2f}%)\")\n",
    "print(f\"\\nAnomaly scores:\")\n",
    "print(f\"  Min: {anomaly_scores.min():.4f}\")\n",
    "print(f\"  Max: {anomaly_scores.max():.4f}\")\n",
    "print(f\"  Mean: {anomaly_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Performance (if labels available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if y_true is not None:\n",
    "    print(\"=\"*60)\n",
    "    print(\"MODEL PERFORMANCE EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # For precision, recall, F1 - we care about detecting anomalies (label = -1)\n",
    "    precision = precision_score(y_true, y_pred, pos_label=-1)\n",
    "    recall = recall_score(y_true, y_pred, pos_label=-1)\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=-1)\n",
    "    \n",
    "    print(f\"\\nüìä Overall Metrics:\")\n",
    "    print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"   Precision: {precision:.4f} (of predicted anomalies, how many were real)\")\n",
    "    print(f\"   Recall:    {recall:.4f} (of real anomalies, how many did we catch)\")\n",
    "    print(f\"   F1 Score:  {f1:.4f} (balance of precision and recall)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No ground truth labels - skipping evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if y_true is not None:\n",
    "    # Confusion Matrix\n",
    "    print(\"\\nüìä Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1, -1])\n",
    "    \n",
    "    print(f\"\\n                  Predicted\")\n",
    "    print(f\"                  Normal  Anomaly\")\n",
    "    print(f\"Actual Normal    {cm[0,0]:7,}  {cm[0,1]:7,}\")\n",
    "    print(f\"Actual Anomaly   {cm[1,0]:7,}  {cm[1,1]:7,}\")\n",
    "    \n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(f\"  True Negatives (correctly identified normal):  {cm[0,0]:,}\")\n",
    "    print(f\"  False Positives (false alarms):                {cm[0,1]:,}\")\n",
    "    print(f\"  False Negatives (missed attacks):              {cm[1,0]:,}\")\n",
    "    print(f\"  True Positives (correctly detected attacks):   {cm[1,1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if y_true is not None:\n",
    "    # Detailed classification report\n",
    "    print(\"\\nüìä Detailed Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Normal', 'Anomaly'], labels=[1, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Heatmap\n",
    "if y_true is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Confusion Matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'Anomaly'],\n",
    "                yticklabels=['Normal', 'Anomaly'],\n",
    "                ax=axes[0])\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    axes[0].set_title('Confusion Matrix')\n",
    "    \n",
    "    # Plot 2: Anomaly Score Distribution\n",
    "    scores_normal = anomaly_scores[y_true == 1]\n",
    "    scores_anomaly = anomaly_scores[y_true == -1]\n",
    "    \n",
    "    axes[1].hist(scores_normal, bins=50, alpha=0.7, label='Normal', color='blue')\n",
    "    axes[1].hist(scores_anomaly, bins=50, alpha=0.7, label='Anomaly', color='red')\n",
    "    axes[1].axvline(x=0, color='black', linestyle='--', label='Decision Boundary')\n",
    "    axes[1].set_xlabel('Anomaly Score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Anomaly Score Distribution')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(anomaly_scores, bins=50, alpha=0.7, color='blue')\n",
    "    plt.axvline(x=0, color='red', linestyle='--', label='Decision Boundary')\n",
    "    plt.xlabel('Anomaly Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Anomaly Score Distribution')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve (if labels available)\n",
    "if y_true is not None:\n",
    "    # Convert labels for ROC (1 = anomaly, 0 = normal)\n",
    "    y_true_binary = (y_true == -1).astype(int)\n",
    "    scores_for_roc = -anomaly_scores\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true_binary, scores_for_roc)\n",
    "    auc_score = roc_auc_score(y_true_binary, scores_for_roc)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {auc_score:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.title('ROC Curve - Anomaly Detection')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä AUC Score: {auc_score:.4f}\")\n",
    "    print(\"   (1.0 = perfect, 0.5 = random, <0.5 = worse than random)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory if needed\n",
    "results_dir = os.path.dirname(RESULTS_SAVE_PATH)\n",
    "if results_dir and not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    print(f\"Created directory: {results_dir}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = test_df.copy()\n",
    "results_df['Prediction'] = np.where(y_pred == 1, 'Normal', 'Anomaly')\n",
    "results_df['Anomaly_Score'] = anomaly_scores\n",
    "results_df['Predicted_Label'] = y_pred\n",
    "\n",
    "if y_true is not None:\n",
    "    results_df['Correct'] = (y_pred == y_true)\n",
    "\n",
    "# Save to CSV\n",
    "print(f\"\\nSaving results to: {RESULTS_SAVE_PATH}\")\n",
    "results_df.to_csv(RESULTS_SAVE_PATH, index=False)\n",
    "print(\"‚úÖ Results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview saved results\n",
    "print(\"\\nPreview of saved results:\")\n",
    "display(results_df[['Prediction', 'Anomaly_Score', 'Predicted_Label']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE COMPLETE - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Test Dataset:\")\n",
    "print(f\"   - Samples: {X_test.shape[0]:,}\")\n",
    "print(f\"   - Features: {X_test.shape[1]}\")\n",
    "print(f\"\\nüîÆ Predictions:\")\n",
    "print(f\"   - Normal:  {(y_pred == 1).sum():,}\")\n",
    "print(f\"   - Anomaly: {(y_pred == -1).sum():,}\")\n",
    "print(f\"   - Prediction time: {prediction_time:.2f} seconds\")\n",
    "\n",
    "if y_true is not None:\n",
    "    print(f\"\\nüìà Performance:\")\n",
    "    print(f\"   - Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"   - Precision: {precision:.4f}\")\n",
    "    print(f\"   - Recall:    {recall:.4f}\")\n",
    "    print(f\"   - F1 Score:  {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nüíæ Results saved to: {RESULTS_SAVE_PATH}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
